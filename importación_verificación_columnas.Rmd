---
title: "Importación de Datos y Verificación de Títulos de Variables"
author: "Javier Jácome"
date: "16 de octubre de 2018"
output:
  html_document:
    depth: 3
    highlight: tango
    number_sections: yes
    theme: lumen
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción.

El presente documento registra los procesos para importar a PostgreSQL las tablas necesarias para la actualización de SICOLE en octubre de 2018. También contiene las líneas de código con las que se verificó que el nombre de las variables coincidiera en las tablas que utiliza el aplicativo y las actuales.  Es importante precisar que esta importación no incluye la conexión y descarga al equipo de las bases de datos, realizada mediante Pentaho.  

# Importación de archivos a repositorio en PostgreSQL. 

Por órden, por la facilidad de realizar directamente consultas en SQL y, eventualmente, para realizar el proceso desde otros equipos, todos los archivos utilizados se cargan a una base e datos en PosgreSQL.

## Instalación de paquetes necesarios

Los paquetes se cargan con las siguientes líneas: 

```{r, warning=FALSE, message=FALSE, include=FALSE}
paquetes <-c("audio", "RPostgreSQL", "tidyverse", "beepr","sf","parallel",  "data.table", 
             "spatialEco","Hmisc", "plyr", "sp", "openxlsx")
nuevos.paquetes<-paquetes[!(paquetes %in% installed.packages()[,"Package"])]
if(length(nuevos.paquetes)) install.packages(nuevos.paquetes)
lapply(paquetes, require, character.only=TRUE)  
```

## Configuración de conexion a PostgreSQL.

Se define el esquema en PostgreSQL dónde quedarán almacenadas las tablas. 

```{r}
options(scipen=999)
drv<-dbDriver("PostgreSQL")
con<-dbConnect(drv,dbname="SIEF",port=5432,user="postgres",
               password="javier",host="localhost")
dbSendQuery(con, "set search_path to sief")
```


## Importación de tablas de base de datos SIEF.

Para importar las tablas se comienza definiendo la ruta dónde están ubicadas. A continuación se seleccionan todos los objetos que tengan ".txt" en su nombre.  Finalmente, con un ciclo for se carga cada una de ellas y luego se exporta a PostgreSQL con los títulos en minúscula, para que no generen problema en el envío de instrucciones SQL. 

```{r, eval=FALSE}
setwd("C:\\pdi-ce-8.1.0.0-365\\data-integration\\Datos")
tablas<-dir()[grep(".txt",dir())]
for(i in 1: length(tablas)){
assign(gsub(".txt", "", tablas[i]),fread(tablas[i]))
setnames(get(gsub(".txt", "", tablas[i])), old=colnames(get(gsub(".txt", "", tablas[i]))),
         new=tolower(colnames(get(gsub(".txt", "", tablas[i])))))
dbWriteTable(con, gsub(".txt", "", tablas[i]), get(gsub(".txt", "", tablas[i])), overwrite=TRUE)
}
beep()
```


## Importación de Directorio de Sedes Educativas

El proceso de importación de la tabla del Directorio de Sedes Educativas de la Dirección de Geoestadística, que contiene las coordenadas de cada una de ellas, se realiza de la siguiente forma: 

```{r}
setwd("D:\\Proyectos\\2018_09_24_Actualización_SICOLE")
dir2<-read.xlsx("EDUCACION_DIRECTORIO20170526_NORMALIZADO_COORDENADAS.xlsx" , sheet=1)
dbWriteTable(con, "dir2", dir2, overwrite=TRUE)
```

Las tablas con los cálculos para 2016, que sirven tanto para extraer la variable de Puntaje en Saber 11 como los nombres de las columnas, se importan así: 


```{r}
setwd("D:\\Proyectos\\2018_09_24_Actualización_SICOLE")
sede<-fread("SICOLE_TABLA_SEDE.csv")
dbWriteTable(con, "sedes2016", sede, overwrite=TRUE)
jornada<-fread("SICOLE_TABLA_JORNADA.csv")
dbWriteTable(con, "jornadas2016", jornada, overwrite=TRUE)
```


# Comprobación de nombres de las columnas.

Si bien la primera parte de este documento se realiza antes del cálculo de variables de SICOLE, esta se realiza al final para verificar, en qué medida, las tablas que tiene el aplicativo en su versión actual difieren de las que se van a entregar. 

Se comienza importando las tablas de Sedes y de Jornadas para los dos periodos. 

```{r}
dbListTables(con)
```


```{r}
sede1<-dbReadTable(con, "sedes2016")
jornada1<-dbReadTable(con, "jornadas2016")
sede2<-dbReadTable(con, "sedes2017")
jornada2<-dbReadTable(con, "jornadas2017")
```

La estrategia ahora consiste en convertir los encabezados de las tablas en dataframes y compararlos con un full outer join.  Las herramientas de SQL para R no permiten realizar este tipo de cálculo directamente, por lo que las tablas se exportan a dicho gestor de bases de datos y los cálculos se realizan desde ahí.  Para las sedes el proceso es el siguiente: 

```{r}
colnames(sede1)<-tolower(colnames(sede1))
sede1<-as.data.frame(colnames(sede1))
sede2<-as.data.frame(colnames(sede2))

names(sede1)<-"sede1"
names(sede2)<-"sede2"

dbWriteTable(con, "sede1", sede1, overwrite=TRUE)
dbWriteTable(con, "sede2", sede2, overwrite=TRUE)

dbSendQuery(con, "drop table sede3")
dbSendQuery(con, "create table sede3 as select sede1, sede2
            from sede1 full outer join sede2
            on sede1=sede2")

sede3<-dbReadTable(con, "sede3")
faltan<-sede3%>%filter(is.na(sede1)|is.na(sede2))
faltan
```

Para jornadas se utiliza un script similar:



```{r}
colnames(jornada1)<-tolower(colnames(jornada1))
colnames(jornada2)<-tolower(colnames(jornada2))
jornada1<-as.data.frame(colnames(jornada1))
jornada2<-as.data.frame(colnames(jornada2))
names(jornada1)<-"jornada1"
names(jornada2)<-"jornada2"


dbWriteTable(con, "jornada1", jornada1, overwrite=TRUE)
dbWriteTable(con, "jornada2", jornada2, overwrite=TRUE)

dbSendQuery(con,"drop table jornada3")
dbSendQuery(con, "create table jornada3 as select jornada1, jornada2
            from jornada1 full outer join jornada2
            on jornada1=jornada2")
dbReadTable(con, "jornada3")
jornada3<-dbReadTable(con, "jornada3")

faltan.jornada<-jornada3%>%filter(is.na(jornada1)|is.na(jornada2))
faltan.jornada
```




